{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f0b1a62",
   "metadata": {},
   "source": [
    "**Tabla de contenido**\n",
    "\n",
    "- [Preparación entorno spark](#Preparacion-entorno-spark)\n",
    "- [Lectura de datos](#Lectura-de-datos)\n",
    "- [Limpieza y transformacion](#Limpieza-y-transformacion)\n",
    "    - [Duplicados](#Duplicados)\n",
    "    - [Convertir columnas a su respectivo formato](#Convertir-columnas-a-su-respectivo-formato)\n",
    "    - [Valores faltantes](#Valores-faltantes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20673ecc",
   "metadata": {},
   "source": [
    "# Preparacion entorno spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de96ac8c",
   "metadata": {},
   "source": [
    "Vamos a preparar el entorno de trabajo con las siguientes configuraciones:\n",
    "\n",
    "1. Importaciones:\n",
    "\n",
    "- `SparkContext:` Punto de entrada principal para funcionalidades de Spark\n",
    "- `SparkSession:` Interfaz unificada para trabajar con datos en Spark\n",
    "\n",
    "\n",
    "2. `sc = SparkContext('local')`:\n",
    "\n",
    "- Crea un contexto de Spark en modo local. \n",
    "- Significa que Spark correrá en tu máquina, usando todos los núcleos disponibles.\n",
    "- Ideal para desarrollo y pruebas\n",
    "3. `spark = SparkSession(sc):`\n",
    "\n",
    "- Crea una sesión de Spark usando el contexto previamente creado\n",
    "- Permite trabajar con DataFrames y realizar operaciones SQL\n",
    "- Es la forma moderna de interactuar con Spark (reemplaza RDDs antiguos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5aa5d3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/22 17:53:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "sc = SparkContext('local')\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aea5f11",
   "metadata": {},
   "source": [
    "# Lectura de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bf4319",
   "metadata": {},
   "source": [
    "Vamos a leer el dataframe y lo almacenaremos en el cache por la siguientes razones:\n",
    "1. Estaremos llamando el dataframe muchas veces ya que lo vamos a limpiar. Si no cacheas, Spark recalculará desde el origen en cada paso intermedio (ineficiente).\n",
    "2. El dataframe es pequeño, por lo cual cabe en memoria.\n",
    "\n",
    "`Beneficios`\n",
    "\n",
    "- Velocidad: Operaciones como dropDuplicates() o groupBy() serán más rápidas al evitar recalcular desde el origen.\n",
    "\n",
    "- Simplicidad: No necesitas preocuparte por repetir lecturas del archivo.\n",
    "\n",
    "- Debugging: Puedes revisar resultados intermedios con show() sin penalización de rendimiento.\n",
    "\n",
    "`¿Cuándo NO cachear?`\n",
    "\n",
    "- Si el DataFrame solo se usa una vez.\n",
    "- Si haces una sola acción después de todas las transformaciones.\n",
    "\n",
    "`Para DataFrames grandes que no caben en memoria pero requieren reutilización:̣`\n",
    "\n",
    "- Usa MEMORY_AND_DISK como nivel de persistencia.\n",
    "\n",
    "- Reparticiona para manejar trozos más pequeños.\n",
    "\n",
    "- Monitoriza el uso de recursos en la UI de Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cb13680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+----------------+--------+----------+--------------------+\n",
      "|Store_ID|          City|       Open_Date|Zip_Code|     Phone|               Email|\n",
      "+--------+--------------+----------------+--------+----------+--------------------+\n",
      "|       1| *San Antonio*|24/07/2003 21:45|   78015|2136875667| *store1@retail.com*|\n",
      "|       2| @San Antonio!| 27/10/2005 7:01|   78201|2135167236| @store2@retail.com!|\n",
      "|       3| #San Antonio#|01/02/2007 16:59|   78112|2123501192|#store3@walmart.com#|\n",
      "|       4|     &Houston&| 17/07/2004 0:28|   77001|3129778178|&store4@walmart.com&|\n",
      "|       5|     !Houston!| 23/04/2009 1:08|   77002|7136191883|  !store5@store.com!|\n",
      "|       6| $Los Angeles$|10/08/2006 19:42|   90001|2127216342|  $store6@store.com$|\n",
      "|       7|     %Phoenix%| 16/10/2007 8:02|   60601|7138018755|  %store7@store.com%|\n",
      "|       8|   #San Diego#| 10/03/2002 5:40|   94023|2125445446| #store8@retail.com#|\n",
      "|       9|    @New York@| 24/07/2007 4:31|   10001|3122242101|  @store9@store.com@|\n",
      "|      10|!Philadelphia!|21/05/2007 13:40|   19019|2135196422|!store10@walmart....|\n",
      "+--------+--------------+----------------+--------+----------+--------------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "file_path = lambda file: os.path.join(os.getcwd(),'data',file)\n",
    "df = spark.read.csv(file_path('Table_Store.csv'),sep=',',header=True)\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46752634",
   "metadata": {},
   "source": [
    "Veamos la estructura del dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4627b4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Store_ID: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Open_Date: string (nullable = true)\n",
      " |-- Zip_Code: string (nullable = true)\n",
      " |-- Phone: string (nullable = true)\n",
      " |-- Email: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b94041",
   "metadata": {},
   "source": [
    "Podemos observar lo siguiente:\n",
    "1. Todas las columnas son de tipo String\n",
    "1. Todas las columnas permiten valores nulos `nullable = true`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653436bb",
   "metadata": {},
   "source": [
    "# Limpieza y transformacion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade907a0",
   "metadata": {},
   "source": [
    "La limpieza y transformación de los datos la vamos a realizar siguiendo los siguientes pasos:\n",
    "\n",
    "1. Consultar la cantidad de registros dúplicados\n",
    "2. Convertir las columnas a su respectivo formato.\n",
    "3. Consultar la cantidad de valores faltantes\n",
    "4. Eliminar caracteres especiales\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb772fd",
   "metadata": {},
   "source": [
    "## Duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e48cc723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existen un total de 0 registros dúplicados\n"
     ]
    }
   ],
   "source": [
    "duplicados = df.groupBy(df.columns).count().filter('count > 1').count()\n",
    "print(f\"Existen un total de {duplicados} registros dúplicados\")\n",
    "df= df.dropDuplicates() # elimina los dúplicados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae60172",
   "metadata": {},
   "source": [
    "## Convertir columnas a su respectivo formato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02e3abab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Store_ID: integer (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Open_Date: timestamp (nullable = true)\n",
      " |-- Zip_Code: string (nullable = true)\n",
      " |-- Phone: string (nullable = true)\n",
      " |-- Email: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_timestamp, col\n",
    "\n",
    "columns_to_num = [\"Store_ID\"] # columnas a convertir a entero\n",
    "for col_name in columns_to_num:\n",
    "    df = df.withColumn(col_name, df[col_name].cast(\"int\"))\n",
    "\n",
    "# convierte la clumna Open_Date a datetime \n",
    "df = df.withColumn(\"Open_Date\", to_timestamp(col(\"Open_Date\"), \"dd/MM/yyyy H:mm\"))\n",
    "df = df.sort(\"Store_ID\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c72b8016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+-------------------+--------+----------+--------------------+\n",
      "|Store_ID|          City|          Open_Date|Zip_Code|     Phone|               Email|\n",
      "+--------+--------------+-------------------+--------+----------+--------------------+\n",
      "|       1| *San Antonio*|2003-07-24 21:45:00|   78015|2136875667| *store1@retail.com*|\n",
      "|       2| @San Antonio!|2005-10-27 07:01:00|   78201|2135167236| @store2@retail.com!|\n",
      "|       3| #San Antonio#|2007-02-01 16:59:00|   78112|2123501192|#store3@walmart.com#|\n",
      "|       4|     &Houston&|2004-07-17 00:28:00|   77001|3129778178|&store4@walmart.com&|\n",
      "|       5|     !Houston!|2009-04-23 01:08:00|   77002|7136191883|  !store5@store.com!|\n",
      "|       6| $Los Angeles$|2006-08-10 19:42:00|   90001|2127216342|  $store6@store.com$|\n",
      "|       7|     %Phoenix%|2007-10-16 08:02:00|   60601|7138018755|  %store7@store.com%|\n",
      "|       8|   #San Diego#|2002-03-10 05:40:00|   94023|2125445446| #store8@retail.com#|\n",
      "|       9|    @New York@|2007-07-24 04:31:00|   10001|3122242101|  @store9@store.com@|\n",
      "|      10|!Philadelphia!|2007-05-21 13:40:00|   19019|2135196422|!store10@walmart....|\n",
      "+--------+--------------+-------------------+--------+----------+--------------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeabb417",
   "metadata": {},
   "source": [
    "## Valores faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c0de6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- InMemoryTableScan [Store_ID#76, City#18, Open_Date#77, Zip_Code#20, Phone#21, Email#22]\n",
      "      +- InMemoryRelation [Store_ID#76, City#18, Open_Date#77, Zip_Code#20, Phone#21, Email#22], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "            +- AdaptiveSparkPlan isFinalPlan=false\n",
      "               +- Sort [Store_ID#76 ASC NULLS FIRST], true, 0\n",
      "                  +- Exchange rangepartitioning(Store_ID#76 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=212]\n",
      "                     +- HashAggregate(keys=[Zip_Code#20, Email#22, City#18, Open_Date#19, Phone#21, Store_ID#17], functions=[])\n",
      "                        +- Exchange hashpartitioning(Zip_Code#20, Email#22, City#18, Open_Date#19, Phone#21, Store_ID#17, 200), ENSURE_REQUIREMENTS, [plan_id=209]\n",
      "                           +- HashAggregate(keys=[Zip_Code#20, Email#22, City#18, Open_Date#19, Phone#21, Store_ID#17], functions=[])\n",
      "                              +- FileScan csv [Store_ID#17,City#18,Open_Date#19,Zip_Code#20,Phone#21,Email#22] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/media/luisgarcia/Datos/31. Spark/Spark/data/Table_Store.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Store_ID:string,City:string,Open_Date:string,Zip_Code:string,Phone:string,Email:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.cache()\n",
    "df.explain()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
